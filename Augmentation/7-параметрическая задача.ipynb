{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2895e9af-f64a-401f-ba8d-d5416bb4908f",
   "metadata": {},
   "source": [
    "7-параметрическая задача по определению концентрации ионов $Cu^{2+}$', '$Ni^{2+}$', '$Pb^{2+}$', '$Co^{2+}$', '$Al^{3+}$', '$Cr^{3+}$', '$NO_3^-$ по спектрам ФЛ УТ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb37e4d-5f3a-4b15-9481-1c3cd190c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "#import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import chain\n",
    "import pandas as pd \n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import datetime\n",
    "from torcheval.metrics import R2Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e826ed0-cc2d-464a-a0a7-1308388614b7",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "916b0776-f321-4eba-bce9-64d6eacf38d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDs_2D_dataset(Dataset):\n",
    "    def __init__(self, annotations_file, spec_dir, transform=None, target_transform=None):\n",
    "      '''annotations_file - the file which contains lables of the samples included in training/validation/test sets'''\n",
    "      self.spec_labels = pd.read_csv(annotations_file, sep=',', decimal=\",\").iloc[:,1:] # labels (concentrations for 4 ions) for all the samples from annotation file (Y_ions.csv)\n",
    "      self.spec_number = pd.read_csv(annotations_file, sep=',', decimal=\",\").iloc[:,0] # numbers for all the samples from annotation file (Y_ions.csv)\n",
    "      self.spec_dir = spec_dir # folder where csv files are located\n",
    "      self.transform = transform \n",
    "      self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spec_labels)#length of the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.spec_labels.iloc[idx] # get the label of the sample via sample's index\n",
    "        \n",
    "        sp = np.array(pd.read_csv(self.spec_dir + str(self.spec_number[idx])+'.csv').T.iloc[1:,:], dtype='float32') # get the EEM of the sample via sample's index\n",
    "        #sp = np.array(pd.read_csv(self.spec_dir + str(self.spec_number[idx])+'_CorrectionData'+'.csv', skiprows=38, sep=';', decimal=\",\").T.iloc[1:-1,:], dtype='float32') # get the EEM of the sample via sample's index\n",
    "\n",
    "        sp[sp<0]=0 # here we zero negative values of intensities\n",
    "        spec = torch.from_numpy(sp).unsqueeze(0) # add dimension for channels of cnn\n",
    "        \n",
    "        if self.transform:\n",
    "            spec = self.transform(spec)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return spec, torch.from_numpy(np.array(label, dtype='float32')) # return spectrum and corresponding labels\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a9a1af1-e31f-4dde-a453-427eb7d009a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 7\n",
      "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 4.3270, 0.0000,  ..., 2.6210, 0.0860, 0.0000],\n",
      "         [0.0000, 1.6450, 0.0000,  ..., 3.3950, 1.6760, 0.3390],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\n",
      "torch.Size([1, 201, 27])\n",
      "tensor([[ 4.5000,  1.5000,  3.0000,  3.0000,  6.0000,  0.0000, 39.0000]])\n"
     ]
    }
   ],
   "source": [
    "file_x = '/Users/galinacugreeva/Desktop/УТ 7 параметров/CD_HM_dataset/'#1000_CorrectionData.csv'\n",
    "file_y = '/Users/galinacugreeva/Desktop/УТ 7 параметров/CD_HM_dataset/Y_ions.csv'\n",
    "\n",
    "training_data =  CDs_2D_dataset(file_y, file_x)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True)\n",
    "\n",
    "# смотреть на размерности\n",
    "for i, j in train_dataloader:\n",
    "    print(i.shape[0], i.shape[1], j.shape[0], j.shape[1])\n",
    "    print(i[0]), print(i[0].shape)\n",
    "    print(j)\n",
    "    break\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585bec1-dc0d-41a3-bc00-1f9171beca03",
   "metadata": {},
   "source": [
    "## Calculate mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "945ac695-b87c-472a-9fde-a8e5c8e26628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std(train_dataloader):\n",
    "    mean = 0.0\n",
    "    for specs, _ in train_dataloader:\n",
    "        batch_samples = specs.size(0) \n",
    "        specs = specs.view(batch_samples, specs.size(1), -1)\n",
    "        mean += specs.mean(2).sum(0)\n",
    "    mean = mean / len(train_dataloader.dataset)\n",
    "\n",
    "    var = 0.0\n",
    "    for specs, _ in train_dataloader:\n",
    "        batch_samples = specs.size(0)\n",
    "        specs = specs.view(batch_samples, specs.size(1), -1)\n",
    "        var += ((specs - mean.unsqueeze(1))**2).sum([0,2])\n",
    "    #std = torch.sqrt(var / (len(train_dataloader.dataset)*500*41))\n",
    "    std = torch.sqrt(var / (len(train_dataloader.dataset)*200*27))\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad5246e-0142-4fe2-a4e8-eca7f2e23adf",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3d1901a-0242-4594-9b07-cc0acd57bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class twoD_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5, padding='same'),\n",
    "            # nn.BatchNorm2d(6),\n",
    "            nn.MaxPool2d(2), # 200->100; 27->13\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(3 * 100 * 13, 7)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.CNN(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "252ee234-11a0-4ab1-b7ae-51a49b7b2485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      ">>> Encoder\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 3, 201, 27]              78\n",
      "         MaxPool2d-2           [-1, 3, 100, 13]               0\n",
      "         LeakyReLU-3           [-1, 3, 100, 13]               0\n",
      "           Flatten-4                 [-1, 3900]               0\n",
      "         LeakyReLU-5                 [-1, 3900]               0\n",
      "            Linear-6                    [-1, 7]          27,307\n",
      "================================================================\n",
      "Total params: 27,385\n",
      "Trainable params: 27,385\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.24\n",
      "Params size (MB): 0.10\n",
      "Estimated Total Size (MB): 0.37\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model_twoCNN = twoD_CNN()\n",
    "\n",
    "model_twoCNN = model_twoCNN.to(device)\n",
    "\n",
    "print(\">>> Encoder\")\n",
    "print(summary(model_twoCNN, (1, 201, 27)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5768c6-0540-4cc1-92f0-1bb37afcf94e",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6bc3760-a729-4065-8e30-30bb0390ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    '''\n",
    "    Try resetting model weights to avoid weight leakage.\n",
    "    '''\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe5b5816-8e03-4880-aca0-5498b1f5ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(N, model_name, split_path, optimizer, dataloader, dset):\n",
    "    \n",
    "    #load the model\n",
    "    checkpoint = torch.load(split_path + 'model'+model_name+'.pth')\n",
    "    N.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    #load optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    last_best_epoch = checkpoint['epoch'] #the epoch with minimum loss function that occured during training\n",
    "    loss = checkpoint['loss']\n",
    "    N.eval()\n",
    "\n",
    "    y_ae = np.zeros((1,7))\n",
    "    y_ae_true = np.zeros((1,7))\n",
    "    \n",
    "    #write the outputs of the model\n",
    "    for specs, labels in dataloader:\n",
    "        outputs = N(specs) # get the outputs from the network\n",
    "        outputs[outputs<0]=0 #we zero negative outputs as they are impossible for concentration values\n",
    "        ae = outputs.detach().numpy()\n",
    "        ae_true = labels.detach().numpy()\n",
    "        #np.concatenate((y_ae, ae), axis=0)\n",
    "        y_ae = np.concatenate((y_ae, ae), axis=0) # columns with networks's output for the dataloader\n",
    "        y_ae_true = np.concatenate((y_ae_true, ae_true), axis=0) # columns with true output for the dataloader\n",
    "\n",
    "    a = ['Cu','Ni', 'Pb', 'Al', 'Co', 'Cr','NO3']\n",
    "    #a = ['pH']\n",
    "    pd.DataFrame(y_ae).to_csv(split_path + 'Y_out_'+'_'+dset+'.csv',sep=',', header = a)\n",
    "    pd.DataFrame(y_ae_true).to_csv(split_path + 'Y_true_'+'_'+dset+'.csv',sep=',', header = a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6009559-afd4-4336-9746-4c130c384ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_metrics(model, loader):\n",
    "    \n",
    "    for specs, targets in loader:\n",
    "        preds = model(specs)\n",
    "        preds[preds<0]=0\n",
    "    \n",
    "        loss_mae = nn.L1Loss()\n",
    "        mae = loss_mae(preds, targets)\n",
    "    \n",
    "        loss_rmse = nn.MSELoss()\n",
    "        rmse = loss_rmse(preds, targets)**0.5\n",
    "    \n",
    "        r2 = R2Score()\n",
    "        r2.update(preds, targets)\n",
    "        r2 = r2.compute()\n",
    "    \n",
    "    return mae, rmse, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75885db0-fb1e-43b4-a008-f4ac84968db9",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bb602-9114-4f42-85fb-15b13fb0599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = '/Users/galinacugreeva/Desktop/УТ 7 параметров/' \n",
    "\n",
    "case_name = 'модель с 1 сверточным слоем' + '_' + str(datetime.date.today())\n",
    "#for cat in ['Cu', 'Cr', 'Ni', 'anions']:\n",
    "cnn_2d_path = case_name+'/'#+cat+'/'\n",
    "\n",
    "Y = pd.read_csv(gen_path+'CD_HM_dataset/'+'Y_ions'+'.csv', sep=',')\n",
    "\n",
    "\n",
    "k_folds = [[42,12],[612,45],[72,172],[871,48]] \n",
    "\n",
    "for fold in k_folds:\n",
    "    split_path = gen_path+cnn_2d_path+'split_'+ str(fold[0])+'_'+str(fold[1])+ '/'\n",
    "    os.makedirs(split_path, exist_ok=True)\n",
    "\n",
    "    Y_trn, Y_30 = train_test_split(Y, test_size=0.3, random_state=fold[0])\n",
    "    Y_vld, Y_tst = train_test_split(Y_30, test_size = 0.3333, random_state=fold[1])\n",
    "\n",
    "    a = ['sample_number','Cu','Ni', 'Pb', 'Al', 'Co', 'Cr','NO3']\n",
    "\n",
    "    pd.DataFrame(Y_trn).to_csv(split_path + 'Y_trn'+'.csv',sep=',', index=False, header = a)\n",
    "    pd.DataFrame(Y_vld).to_csv(split_path + 'Y_vld'+'.csv',sep=',', index=False, header = a)\n",
    "    pd.DataFrame(Y_tst).to_csv(split_path + 'Y_tst'+'.csv',sep=',', index=False, header = a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964caa5-b6b8-4ebd-ac11-1d172d268c31",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fc5e0-d094-4167-842f-8abcccaa9765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e18834-6e6f-4b44-8a35-53c179491bfd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c79b6a0-9403-4cff-910d-458a64b179c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train(model, lr, l2_lambda, k_folds, case_name, mnozh_init, epochs_num):\n",
    "\n",
    "    print('Start in', str(datetime.datetime.now()))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "        \n",
    "    loss_function = torch.nn.MSELoss().cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    for fold in k_folds:\n",
    "        \n",
    "        split_path = gen_path+cnn_2d_path+'split_'+ str(fold[0])+'_'+str(fold[1])+ '/'\n",
    "\n",
    "        training_data = CDs_2D_dataset(split_path+'Y_trn'+'.csv',gen_path + 'CD_HM_dataset/')\n",
    "        train_dataloader = DataLoader(training_data, batch_size=256, shuffle=True)\n",
    "        mean, std = mean_std(train_dataloader)\n",
    "    \n",
    "        #Data import and normalization Basic\n",
    "    \n",
    "        training_data = CDs_2D_dataset(split_path+'Y_trn'+'.csv', gen_path + 'CD_HM_dataset/', transform= transforms.Compose([transforms.Normalize(mean, std)]))\n",
    "    \n",
    "        validation_data = CDs_2D_dataset(split_path+'Y_vld'+'.csv', gen_path + 'CD_HM_dataset/', transform= transforms.Compose([transforms.Normalize(mean, std)]))\n",
    "        test_data = CDs_2D_dataset(split_path+'Y_tst'+'.csv', gen_path + 'CD_HM_dataset/', transform= transforms.Compose([transforms.Normalize(mean, std)]))\n",
    "    \n",
    "        train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "        validation_dataloader = DataLoader(validation_data, batch_size=64, shuffle=True)\n",
    "        test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "        print(mean, std)\n",
    "\n",
    "    \n",
    "        for init_number in range(0, mnozh_init): #Это я делаю множественную инициализацию весов сети\n",
    "    \n",
    "            init_path = split_path + str(init_number)+'/'\n",
    "            os.makedirs(init_path, exist_ok=True)\n",
    "    \n",
    "            test_stop = 100 #stopping criterion\n",
    "            max_val_loss = 10000.0\n",
    "\n",
    "            wandb.init(project = case_name)\n",
    "        \n",
    "            split_name = 'split_'+ str(fold[0])+'_'+str(fold[1])\n",
    "            init_name = '_' + str(init_number)\n",
    "            model_name = '_2D_CNN'\n",
    "\n",
    "            wandb.run.name = split_name + init_name + model_name+'_reg_'+str(l2_lambda)\n",
    "            wandb.run.save()\n",
    "    \n",
    "            for epoch_step in range(0, epochs_num, test_stop):\n",
    "                \n",
    "                if epoch_step!=0:\n",
    "                    checkpoint = torch.load(init_path + 'model'+model_name+'.pth')\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                    last_best_epoch = checkpoint['epoch']\n",
    "                    loss = checkpoint['loss']\n",
    "                    model.train()\n",
    "                    \n",
    "                    if last_best_epoch + test_stop > ep:\n",
    "                        for ep in range(epoch_step, epoch_step+test_stop):\n",
    "                            for _, data in enumerate(train_dataloader, 0): # get bacth\n",
    "                                inputs, labels = data # parse batch\n",
    "                                optimizer.zero_grad() # sets the gradients of all optimized tensors to zero.\n",
    "                                outputs = model(inputs) # get outputs\n",
    "\n",
    "                                loss = loss_function(outputs, labels) # calculate loss\n",
    "                                l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                                loss = loss + l2_lambda * l2_norm\n",
    "                                loss.backward() # calculate gradients\n",
    "                            \n",
    "                                optimizer.step() # performs a single optimization step (parameter update).\n",
    "    \n",
    "                            dl = 0\n",
    "                            val_loss = 0.0\n",
    "                            for specs, labels in validation_dataloader: \n",
    "                                val_loss += loss_function(model(specs),labels)\n",
    "                                dl+=1\n",
    "                            val_loss = val_loss/dl\n",
    "                            \n",
    "                            if val_loss.item() <= max_val_loss:\n",
    "                                torch.save({'epoch': ep,\n",
    "                                  'model_state_dict': model.state_dict(),\n",
    "                                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                  'loss': loss}, init_path + 'model'+model_name+'.pth')\n",
    "                                max_val_loss = val_loss.item()\n",
    "                            wandb.log({\"trn_loss\": loss, \"vld_loss\": val_loss})\n",
    "\n",
    "                    else: continue    \n",
    "                print(epoch_step)\n",
    "                if epoch_step==0:\n",
    "                    model.apply(reset_weights)\n",
    "    \n",
    "                    for ep in range(epoch_step, test_stop):\n",
    "                        for _, data in enumerate(train_dataloader, 0): # get bacth\n",
    "                            inputs, labels = data # parse batch\n",
    "                            optimizer.zero_grad() # sets the gradients of all optimized tensors to zero.\n",
    "                            outputs = model(inputs) # get outputs\n",
    "                            loss = loss_function(outputs, labels) # calculate loss\n",
    "                            loss.backward() # calculate gradients\n",
    "                            optimizer.step() # performs a single optimization step (parameter update).\n",
    "    \n",
    "                        dl = 0\n",
    "                        val_loss = 0.0\n",
    "                        for specs, labels in validation_dataloader:\n",
    "                            val_loss += loss_function(model(specs),labels)\n",
    "                            dl+=1\n",
    "                        val_loss = val_loss/dl\n",
    "    \n",
    "                        if val_loss.item() <= max_val_loss:\n",
    "                            torch.save({'epoch': ep,\n",
    "                              'model_state_dict': model.state_dict(),\n",
    "                              'optimizer_state_dict': optimizer.state_dict(),\n",
    "                              'loss': loss}, init_path + 'model'+model_name+'.pth')\n",
    "                            max_val_loss = val_loss.item()\n",
    "                        wandb.log({\"trn_loss\": loss, \"vld_loss\": val_loss})\n",
    "    \n",
    "            write_predictions(model, model_name, init_path, optimizer, train_dataloader, dset = 'trn')\n",
    "            write_predictions(model, model_name, init_path, optimizer, validation_dataloader, dset ='vld')\n",
    "            write_predictions(model, model_name, init_path, optimizer, test_dataloader, dset ='tst')\n",
    "\n",
    "            trn_metrics = calculate_metrics(model, train_dataloader)\n",
    "            vld_metrics = calculate_metrics(model, validation_dataloader)\n",
    "            tst_metrics = calculate_metrics(model, test_dataloader)\n",
    "\n",
    "            wandb.log({\"trn_mae\": float(trn_metrics[0]), \"trn_rmse\": float(trn_metrics[1]), \"trn_r2\": float(trn_metrics[2])})\n",
    "            wandb.log({\"vld_mae\": float(vld_metrics[0]), \"vld_rmse\": float(vld_metrics[1]), \"vld_r2\": float(vld_metrics[2])})\n",
    "            wandb.log({\"tst_mae\": float(tst_metrics[0]), \"tst_rmse\": float(tst_metrics[1]), \"tst_r2\": float(tst_metrics[2])})\n",
    "            \n",
    "            wandb.log({\"epoch\": ep})\n",
    "    \n",
    "            wandb.finish()\n",
    "            \n",
    "            print(epoch_step, fold)\n",
    "            print(str(datetime.datetime.now()))\n",
    "            \n",
    "    print('End in', str(datetime.datetime.now()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f2fbc-7e4f-49da-a83e-c4ff8bdbad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=twoD_CNN(), \n",
    "      lr=0.001, \n",
    "      l2_lambda = 0.001,\n",
    "      k_folds=k_folds,\n",
    "      case_name=case_name,\n",
    "      mnozh_init=1, \n",
    "      epochs_num=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
